{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.llamaindex.ai/en/stable/examples/low_level/oss_ingestion_retrieval.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sentence Transformers as the embedding model\n",
    "\n",
    "- Postgres as the vector store (we support many other vector stores too!)\n",
    "\n",
    "- Llama 2 as the LLM (through llama.cpp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('../env/pinecone.conf')\n",
    "\n",
    "api_key        = config[\"DEFAULT\"][\"PINECONE_API_KEY\"]\n",
    "environment    = config[\"DEFAULT\"][\"PINECONE_ENVIRONMENT\"]\n",
    "openai_api_key = config[\"DEFAULT\"][\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parkhyerin/Study/TIL_RAG/env/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding   # https://docs.llamaindex.ai/en/latest/examples/embeddings/OpenAI.html\n",
    "\n",
    "# huggingface\n",
    "#embed_model = HuggingFaceEmbedding(model_name=\"snunlp/KR-SBERT-V40K-klueNLI-augSTS\")\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"kakaobank/kf-deberta-base\")\n",
    "#embed_model = HuggingFaceEmbedding(model_name=\"intfloat/e5-large-v2\")\n",
    "#embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-m3\")        # sota ?\n",
    "\n",
    "# openai\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "\n",
    "#embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 363 tensors from /Users/parkhyerin/Library/Caches/llama_index/models/llama-2-13b-chat.Q4_0.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q4_0:  281 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 5120\n",
      "llm_load_print_meta: n_embd_v_gqa     = 5120\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 13.02 B\n",
      "llm_load_print_meta: model size       = 6.86 GiB (4.53 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.28 MiB\n",
      "ggml_backend_metal_buffer_from_ptr: allocated buffer, size =   170.20 MiB, ( 1203.92 / 27648.00)\n",
      "llm_load_tensors: offloading 1 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 1/41 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  7023.90 MiB\n",
      "llm_load_tensors:      Metal buffer size =   170.20 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 3900\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M3 Pro\n",
      "ggml_metal_init: picking default device: Apple M3 Pro\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: GGML_METAL_PATH_RESOURCES = nil\n",
      "ggml_metal_init: loading '/Users/parkhyerin/Study/TIL_RAG/env/lib/python3.9/site-packages/llama_cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M3 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 28991.03 MB\n",
      "llama_kv_cache_init:        CPU KV buffer size =  2970.70 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =    76.17 MiB, ( 1284.34 / 27648.00)\n",
      "llama_kv_cache_init:      Metal KV buffer size =    76.17 MiB\n",
      "llama_new_context_with_model: KV self size  = 3046.88 MiB, K (f16): 1523.44 MiB, V (f16): 1523.44 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =    18.65 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   352.33 MiB, ( 1636.67 / 27648.00)\n",
      "llama_new_context_with_model:      Metal compute buffer size =   352.32 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   354.69 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 5\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '40', 'llama.context_length': '4096', 'llama.attention.head_count': '40', 'llama.rope.dimension_count': '128', 'general.file_type': '2', 'llama.feed_forward_length': '13824', 'llama.embedding_length': '5120', 'llama.block_count': '40', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'LLaMA v2'}\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.llama_cpp import LlamaCPP\n",
    "\n",
    "model_url = \"https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF/resolve/main/llama-2-13b-chat.Q4_0.gguf\"\n",
    "#model_url = \"https://huggingface.co/TheBloke/SOLAR-10.7B-Instruct-v1.0-GGUF/resolve/main/solar-10.7b-instruct-v1.0.Q4_0.gguf\"\n",
    "#model_url = \"https://huggingface.co/davidkim205/komt-mistral-7b-v1-gguf/resolve/main/ggml-model-q4_0.gguf\"\n",
    "#model_url = \"https://huggingface.co/davidkim205/komt-Llama-2-13b-hf-ggml/resolve/main/ggml-model-q4_0.bin\"  # gguf_init_from_file: invalid magic characters 'tjgg'\n",
    "#model_url = \"https://huggingface.co/StarFox7/Llama-2-ko-7B-chat-gguf/resolve/main/Llama-2-ko-7B-chat-gguf-q4_0.bin\" # gguf_init_from_file: GGUFv1 is no longer supported. please use a more up-to-date version\n",
    "\n",
    "llm = LlamaCPP(\n",
    "    # You can pass in the URL to a GGUF model to download it automatically\n",
    "    model_url=model_url,\n",
    "    # optionally, you can set the path to a pre-downloaded model instead of model_url\n",
    "    model_path=None,\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=256,\n",
    "    # llama2 has a context window of 4096 tokens, but we set it lower to allow for some wiggle room\n",
    "    context_window=3900,\n",
    "    # kwargs to pass to __call__()\n",
    "    generate_kwargs={},\n",
    "    # kwargs to pass to __init__()\n",
    "    # set to at least 1 to use GPU\n",
    "    model_kwargs={\"n_gpu_layers\": 1},\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "db_name = \"vector_db\"\n",
    "host = \"localhost\"\n",
    "password = \"1234\"\n",
    "port = \"5432\"\n",
    "user = \"local\"\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"postgres\",\n",
    "    host=host,\n",
    "    password=password,\n",
    "    port=port,\n",
    "    user=user,\n",
    ")\n",
    "conn.autocommit = True\n",
    "\n",
    "with conn.cursor() as c:\n",
    "    c.execute(f\"DROP DATABASE IF EXISTS {db_name}\")\n",
    "    c.execute(f\"CREATE DATABASE {db_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import make_url\n",
    "from llama_index.vector_stores.postgres import PGVectorStore\n",
    "\n",
    "vector_store = PGVectorStore.from_params(\n",
    "    database=db_name,\n",
    "    host=host,\n",
    "    password=password,\n",
    "    port=port,\n",
    "    user=user,\n",
    "    table_name=\"table\",\n",
    "    embed_dim=768,  # sbert, kakaobank\n",
    "    #embed_dim=1536,  # openai embedding dimension\n",
    "    #embed_dim=1024,  # bge-m3 embedding dimension\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build an Ingestion Pipeline from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read pdf \n",
    "\n",
    "from pathlib import Path\n",
    "from llama_index.readers.file import PyMuPDFReader\n",
    "\n",
    "#loader = PyMuPDFReader()\n",
    "#documents = loader.load(file_path=\"../data/sample.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data directory\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "# https://docs.llamaindex.ai/en/stable/examples/data_connectors/simple_directory_reader.html\n",
    "\n",
    "required_exts = [\".txt\"]\n",
    "\n",
    "reader = SimpleDirectoryReader(\n",
    "    input_dir=\"../data\",\n",
    "    required_exts=required_exts,\n",
    "    recursive=True,\n",
    ")\n",
    "\n",
    "documents = reader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='886d3513-80f6-4f46-80cd-576d2b26f35a', embedding=None, metadata={'file_path': '../data/1.txt', 'file_name': '1.txt', 'file_type': 'text/plain', 'file_size': 3465, 'creation_date': '2024-02-23', 'last_modified_date': '2024-02-23', 'last_accessed_date': '2024-02-23'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='생성형 인공지능(AI) 업계에서 오픈AI와 패권을 다투는 구글이 ‘오픈소스(개방형) AI’ 진영으로 한 걸음 이동했다. 업계는 챗GPT의 기반 모델인 ‘GPT4’의 어떤 것도 공개하지 않는 오픈AI와 구글의 기술 경쟁이 어떤 결말을 가져올지 주목하고 있다.\\n\\n구글은 21일(현지시간) 거대언어모델(LLM) ‘젬마’를 오픈소스로 공개했다. 이에 따라 개별 연구자나 개발자, 기업, 연구기관 등은 젬마를 자유롭게 활용할 수 있다. 젬마는 구글의 AI 모델 ‘제미나이’의 경량형이라고 생각하면 쉽다. AI 모델의 규모를 판단하는 매개변수가 200억개, 700억개인 두 개의 모델로 공개됐다. 제미나이의 매개변수는 1조 7800억개 이상으로 추정된다.\\n\\n오픈소스는 AI 이외의 분야에서도 정보기술(IT) 개발자들이 널리 공감하는 개념이다. AI 모델을 비롯한 컴퓨터 프로그램의 소스코드 등 모든 개발 자산을 누구나 다운로드할 수 있는 공간에 공유하는 것이다. 생성형 AI 시대에 와서도 오픈소스는 많은 지지를 얻었다. 기존 오픈소스 플랫폼 ‘깃허브’나 오픈소스 AI 모델 플랫폼인 ‘허깅페이스’에 수많은 AI 모델들이 공유됐다. 메타(페이스북)는 지난해 빅테크 중 가장 먼저 ‘라마’(LlaMa)라는 생성 AI 모델을 오픈소스로 공개했다. 국내에선 업스테이지의 ‘솔라’, 모레의 ‘모모’ 등이 오픈소스 AI 중 높은 성능 순위를 기록했다.\\n\\n오픈소스 AI는 수많은 전문가들이 모델의 강약점을 함께 고민하고 성능과 안전성을 개선할 수 있다. 특히 기본 모델을 공유하면 가난한 국가나 영세 기관, 개인 개발자 등도 이를 기반으로 AI 서비스를 개발하거나 변형해 새 AI 모델도 만들 수 있다. AI 시대 기술 불평등, 종속 문제를 해소할 ‘AI 민주화’ 방안으로도 꼽힌다.\\n\\n오픈AI는 GPT4의 매개변수조차 공개하지 않는 등 폐쇄 정책으로 일관하고 있다. AI 시대 초반에 기술 우위를 유지해 구글에 앞서기 위해서다. 지금까지는 효과적이다. 최근엔 단 몇 줄의 명령어로 영화 속 한 장면 같은 동영상을 만들어 내는 생성 AI ‘소라’를 공개해 업계를 놀라게 했다.\\n\\n당초 오픈소스 정신을 실천해 온 구글은 오픈AI의 챗GPT 등장으로 기술 경쟁이 과열되면서 폐쇄적인 입장이 됐다가 최근 오픈소스 진영으로 돌아가고 있다. AI 모델을 공개하는 것이 독점하는 것보다 오히려 플랫폼 장악에 유리하다고 판단했기 때문이다. 뉴욕타임스에 따르면 구글 측은 “외부 개발자 커뮤니티를 다시 참여시키고 구글의 기반 모델이 최신 AI 구축 방식의 업계 표준이 될 수 있기를 희망한다”고 밝혔다.\\n\\n최근 철저하게 폐쇄 전략을 구사했던 애플이 미국과 유럽 규제당국의 제재를 잇달아 받고 있어 오픈AI의 폐쇄적인 정책도 비슷한 우려를 낳고 있다.\\n\\n업계 관계자는 “아직 너무나 초기이지만 지난달 서비스를 시작한 오픈AI의 ‘GPT 스토어’는 애플의 앱스토어와 비슷하다”며 “플랫폼을 장악한 뒤 독과점이나 특허 침해 등 문제를 일으키지 않는다는 보장이 없다”고 말했다.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9db2f799-4c8d-43ba-b062-34df3b739b68', embedding=None, metadata={'file_path': '../data/2.txt', 'file_name': '2.txt', 'file_type': 'text/plain', 'file_size': 2495, 'creation_date': '2024-02-23', 'last_modified_date': '2024-02-23', 'last_accessed_date': '2024-02-23'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='현재까지의 초전도체는 영하 269도부터 영하 183도까지라는 극저온 환경에서 초전도 현상을 보였습니다. 2019년에는 영하 23도, 압력 1700억 Pa에서 초전도 현상을 보이는 란타늄 수소 화합물 만들기도 했습니다. 이때의 환경은 초고압 환경입니다. 1기압이 101325 Pa이니까 상상할 수 없는 높은 압력입니다.만약 상온 상압에서도 초전도체로 동작하는 물질이 생긴다면 에너지 혁명이 일어날 것입니다. 상온 상압 초전도체를 개발한 사람은 의심할 것 없이 노벨상입니다. 그러므로 2023년 7월에 대한민국에서 상온, 상압의 초전도체를 만들었다는 뉴스는 충분히 세상을 놀라게 할 만합니다. 그 초전도체가 바로 LK-99입니다.\\n\\n퀀텀에너지 연구소는 논문과 함께 초전도체라고 주장하는 LK-99의 영상을 공개했습니다. 자석 위에 동전 모양의 검은 물체가 비스듬히 들려 있습니다. 완전 공중부양이 아니라 한쪽 면이 자석위에 살짝 닿아있는데요. 어쨌거나 뜨다시피 한 모습은 놀랍습니다. 이 물질이 바로 초전도체 물질이라고 주장하는 LK-99입니다.2028년 8월 현재 세계의 유명한 대학과 연구소들은 상온 상압 초전도체인 LK-99를 검증하고 있는 중입니다. 예를들어 미국의 프린스턴 대학, 독일의 막스플랑크 연구소는 LK-99가 특이한 성질을 보였지만 초전도체는 아닌듯하다는 결론을 내놓았습니다. LK-99가 초전도체가 아니다라고 단정한 것은 아닙니다. 다만 연구자들을 설득할 만한 충분한 데이터와 설명이 부족하다는 것입니다. 또 다른 과학자들의 검증결과가 기다려집니다.\\n\\n2023년 7월 LK-99라는 초전도체에 대한 논문이 아카이브에 올라왔고, 그 이후로 초전도체 뉴스가 뜨겁습니다. LK-99라는 초전도체 개발 가능성은 초전도체 활용 분야로 관심이 갔고, 즉시 주식시장에 그 관심이 반영되었습니다.올해 시장 최대 테마주는 초전도체 관련 주입니다. 초전도체 응용기술을 가졌다는 어떤 기업은 2023년 7월31일부터 상한가 3회로 사흘만에 2배가 올랐습니다. 거래정지 하루가 있었고, 그 다음날엔 하한가를 쳐서 그야말로 주가가 롤러코스터를 타고 있습니다. 주식투자에 각별한 주의가 필요해 보입니다.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Use a Text Splitter to Split Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_parser = SentenceSplitter(\n",
    "    chunk_size=1024,\n",
    "    # separator=\" \",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks = []\n",
    "# maintain relationship with source doc index, to help inject doc metadata in (3)\n",
    "doc_idxs = []\n",
    "for doc_idx, doc in enumerate(documents):\n",
    "    cur_text_chunks = text_parser.split_text(doc.text)\n",
    "    text_chunks.extend(cur_text_chunks)\n",
    "    doc_idxs.extend([doc_idx] * len(cur_text_chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Manually Construct Nodes from Text Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import TextNode\n",
    "\n",
    "nodes = []\n",
    "for idx, text_chunk in enumerate(text_chunks):\n",
    "    node = TextNode(\n",
    "        text=text_chunk,\n",
    "    )\n",
    "    src_doc = documents[doc_idxs[idx]]\n",
    "    node.metadata = src_doc.metadata\n",
    "    nodes.append(node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Generate Embeddings for each Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in nodes:\n",
    "    node_embedding = embed_model.get_text_embedding(\n",
    "        node.get_content(metadata_mode=\"all\")\n",
    "    )\n",
    "    node.embedding = node_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Load Nodes into a Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['66f11f29-7609-4aec-a8c8-9c371ac92408',\n",
       " '175294ce-92e5-4fe1-8bfb-7a491d537d84',\n",
       " '8567195a-c4b3-48e5-add1-c7000d5e2c60']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.add(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Retrieval Pipeline from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_str = \"젬마가 뭔가요?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Generate a Query Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embedding = embed_model.get_query_embedding(query_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Query the Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct vector store query\n",
    "from llama_index.core.vector_stores import VectorStoreQuery\n",
    "\n",
    "query_mode = \"default\"\n",
    "# query_mode = \"sparse\"\n",
    "# query_mode = \"hybrid\"\n",
    "\n",
    "vector_store_query = VectorStoreQuery(\n",
    "    query_embedding=query_embedding, similarity_top_k=2, mode=query_mode\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재까지의 초전도체는 영하 269도부터 영하 183도까지라는 극저온 환경에서 초전도 현상을 보였습니다. 2019년에는 영하 23도, 압력 1700억 Pa에서 초전도 현상을 보이는 란타늄 수소 화합물 만들기도 했습니다. 이때의 환경은 초고압 환경입니다. 1기압이 101325 Pa이니까 상상할 수 없는 높은 압력입니다.만약 상온 상압에서도 초전도체로 동작하는 물질이 생긴다면 에너지 혁명이 일어날 것입니다. 상온 상압 초전도체를 개발한 사람은 의심할 것 없이 노벨상입니다. 그러므로 2023년 7월에 대한민국에서 상온, 상압의 초전도체를 만들었다는 뉴스는 충분히 세상을 놀라게 할 만합니다. 그 초전도체가 바로 LK-99입니다.\n",
      "\n",
      "퀀텀에너지 연구소는 논문과 함께 초전도체라고 주장하는 LK-99의 영상을 공개했습니다. 자석 위에 동전 모양의 검은 물체가 비스듬히 들려 있습니다. 완전 공중부양이 아니라 한쪽 면이 자석위에 살짝 닿아있는데요. 어쨌거나 뜨다시피 한 모습은 놀랍습니다. 이 물질이 바로 초전도체 물질이라고 주장하는 LK-99입니다.2028년 8월 현재 세계의 유명한 대학과 연구소들은 상온 상압 초전도체인 LK-99를 검증하고 있는 중입니다. 예를들어 미국의 프린스턴 대학, 독일의 막스플랑크 연구소는 LK-99가 특이한 성질을 보였지만 초전도체는 아닌듯하다는 결론을 내놓았습니다. LK-99가 초전도체가 아니다라고 단정한 것은 아닙니다. 다만 연구자들을 설득할 만한 충분한 데이터와 설명이 부족하다는 것입니다. 또 다른 과학자들의 검증결과가 기다려집니다.\n",
      "\n",
      "2023년 7월 LK-99라는 초전도체에 대한 논문이 아카이브에 올라왔고, 그 이후로 초전도체 뉴스가 뜨겁습니다. LK-99라는 초전도체 개발 가능성은 초전도체 활용 분야로 관심이 갔고, 즉시 주식시장에 그 관심이 반영되었습니다.올해 시장 최대 테마주는 초전도체 관련 주입니다. 초전도체 응용기술을 가졌다는 어떤 기업은 2023년 7월31일부터 상한가 3회로 사흘만에 2배가 올랐습니다. 거래정지 하루가 있었고, 그 다음날엔 하한가를 쳐서 그야말로 주가가 롤러코스터를 타고 있습니다. 주식투자에 각별한 주의가 필요해 보입니다.\n"
     ]
    }
   ],
   "source": [
    "# returns a VectorStoreQueryResult\n",
    "query_result = vector_store.query(vector_store_query)\n",
    "print(query_result.nodes[0].get_content())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Parse Result into a Set of Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import NodeWithScore\n",
    "from typing import Optional\n",
    "\n",
    "nodes_with_scores = []\n",
    "for index, node in enumerate(query_result.nodes):\n",
    "    score: Optional[float] = None\n",
    "    if query_result.similarities is not None:\n",
    "        score = query_result.similarities[index]\n",
    "    nodes_with_scores.append(NodeWithScore(node=node, score=score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Put into a Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import QueryBundle\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from typing import Any, List\n",
    "\n",
    "\n",
    "class VectorDBRetriever(BaseRetriever):\n",
    "    \"\"\"Retriever over a postgres vector store.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_store: PGVectorStore,\n",
    "        embed_model: Any,\n",
    "        query_mode: str = \"default\",\n",
    "        similarity_top_k: int = 2,\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "        self._vector_store = vector_store\n",
    "        self._embed_model = embed_model\n",
    "        self._query_mode = query_mode\n",
    "        self._similarity_top_k = similarity_top_k\n",
    "        super().__init__()\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve.\"\"\"\n",
    "        query_embedding = embed_model.get_query_embedding(\n",
    "            query_bundle.query_str\n",
    "        )\n",
    "        vector_store_query = VectorStoreQuery(\n",
    "            query_embedding=query_embedding,\n",
    "            similarity_top_k=self._similarity_top_k,\n",
    "            mode=self._query_mode,\n",
    "        )\n",
    "        query_result = vector_store.query(vector_store_query)\n",
    "\n",
    "        nodes_with_scores = []\n",
    "        for index, node in enumerate(query_result.nodes):\n",
    "            score: Optional[float] = None\n",
    "            if query_result.similarities is not None:\n",
    "                score = query_result.similarities[index]\n",
    "            nodes_with_scores.append(NodeWithScore(node=node, score=score))\n",
    "\n",
    "        return nodes_with_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = VectorDBRetriever(\n",
    "    vector_store, embed_model, query_mode=\"default\", similarity_top_k=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plug this into our RetrieverQueryEngine to synthesize a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "query_engine = RetrieverQueryEngine.from_args(retriever, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   14379.46 ms\n",
      "llama_print_timings:      sample time =       2.74 ms /    37 runs   (    0.07 ms per token, 13513.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =  110117.09 ms /  3233 tokens (   34.06 ms per token,    29.36 tokens per second)\n",
      "llama_print_timings:        eval time =    4004.33 ms /    36 runs   (  111.23 ms per token,     8.99 tokens per second)\n",
      "llama_print_timings:       total time =  114188.84 ms /  3269 tokens\n"
     ]
    }
   ],
   "source": [
    "#query_str = \"How does Llama 2 perform compared to other open-source models?\"\n",
    "query_str = \"젬마가 무엇인가요?\"\n",
    "\n",
    "response = query_engine.query(query_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "젬마는 구글의 생성형 AI 모델(LLM)입니다.\n"
     ]
    }
   ],
   "source": [
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재까지의 초전도체는 영하 269도부터 영하 183도까지라는 극저온 환경에서 초전도 현상을 보였습니다. 2019년에는 영하 23도, 압력 1700억 Pa에서 초전도 현상을 보이는 란타늄 수소 화합물 만들기도 했습니다. 이때의 환경은 초고압 환경입니다. 1기압이 101325 Pa이니까 상상할 수 없는 높은 압력입니다.만약 상온 상압에서도 초전도체로 동작하는 물질이 생긴다면 에너지 혁명이 일어날 것입니다. 상온 상압 초전도체를 개발한 사람은 의심할 것 없이 노벨상입니다. 그러므로 2023년 7월에 대한민국에서 상온, 상압의 초전도체를 만들었다는 뉴스는 충분히 세상을 놀라게 할 만합니다. 그 초전도체가 바로 LK-99입니다.\n",
      "\n",
      "퀀텀에너지 연구소는 논문과 함께 초전도체라고 주장하는 LK-99의 영상을 공개했습니다. 자석 위에 동전 모양의 검은 물체가 비스듬히 들려 있습니다. 완전 공중부양이 아니라 한쪽 면이 자석위에 살짝 닿아있는데요. 어쨌거나 뜨다시피 한 모습은 놀랍습니다. 이 물질이 바로 초전도체 물질이라고 주장하는 LK-99입니다.2028년 8월 현재 세계의 유명한 대학과 연구소들은 상온 상압 초전도체인 LK-99를 검증하고 있는 중입니다. 예를들어 미국의 프린스턴 대학, 독일의 막스플랑크 연구소는 LK-99가 특이한 성질을 보였지만 초전도체는 아닌듯하다는 결론을 내놓았습니다. LK-99가 초전도체가 아니다라고 단정한 것은 아닙니다. 다만 연구자들을 설득할 만한 충분한 데이터와 설명이 부족하다는 것입니다. 또 다른 과학자들의 검증결과가 기다려집니다.\n",
      "\n",
      "2023년 7월 LK-99라는 초전도체에 대한 논문이 아카이브에 올라왔고, 그 이후로 초전도체 뉴스가 뜨겁습니다. LK-99라는 초전도체 개발 가능성은 초전도체 활용 분야로 관심이 갔고, 즉시 주식시장에 그 관심이 반영되었습니다.올해 시장 최대 테마주는 초전도체 관련 주입니다. 초전도체 응용기술을 가졌다는 어떤 기업은 2023년 7월31일부터 상한가 3회로 사흘만에 2배가 올랐습니다. 거래정지 하루가 있었고, 그 다음날엔 하한가를 쳐서 그야말로 주가가 롤러코스터를 타고 있습니다. 주식투자에 각별한 주의가 필요해 보입니다.\n"
     ]
    }
   ],
   "source": [
    "print(response.source_nodes[0].get_content())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NodeWithScore(node=TextNode(id_='8567195a-c4b3-48e5-add1-c7000d5e2c60', embedding=None, metadata={'file_path': '../data/2.txt', 'file_name': '2.txt', 'file_type': 'text/plain', 'file_size': 2495, 'creation_date': '2024-02-23', 'last_modified_date': '2024-02-23', 'last_accessed_date': '2024-02-23'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='현재까지의 초전도체는 영하 269도부터 영하 183도까지라는 극저온 환경에서 초전도 현상을 보였습니다. 2019년에는 영하 23도, 압력 1700억 Pa에서 초전도 현상을 보이는 란타늄 수소 화합물 만들기도 했습니다. 이때의 환경은 초고압 환경입니다. 1기압이 101325 Pa이니까 상상할 수 없는 높은 압력입니다.만약 상온 상압에서도 초전도체로 동작하는 물질이 생긴다면 에너지 혁명이 일어날 것입니다. 상온 상압 초전도체를 개발한 사람은 의심할 것 없이 노벨상입니다. 그러므로 2023년 7월에 대한민국에서 상온, 상압의 초전도체를 만들었다는 뉴스는 충분히 세상을 놀라게 할 만합니다. 그 초전도체가 바로 LK-99입니다.\\n\\n퀀텀에너지 연구소는 논문과 함께 초전도체라고 주장하는 LK-99의 영상을 공개했습니다. 자석 위에 동전 모양의 검은 물체가 비스듬히 들려 있습니다. 완전 공중부양이 아니라 한쪽 면이 자석위에 살짝 닿아있는데요. 어쨌거나 뜨다시피 한 모습은 놀랍습니다. 이 물질이 바로 초전도체 물질이라고 주장하는 LK-99입니다.2028년 8월 현재 세계의 유명한 대학과 연구소들은 상온 상압 초전도체인 LK-99를 검증하고 있는 중입니다. 예를들어 미국의 프린스턴 대학, 독일의 막스플랑크 연구소는 LK-99가 특이한 성질을 보였지만 초전도체는 아닌듯하다는 결론을 내놓았습니다. LK-99가 초전도체가 아니다라고 단정한 것은 아닙니다. 다만 연구자들을 설득할 만한 충분한 데이터와 설명이 부족하다는 것입니다. 또 다른 과학자들의 검증결과가 기다려집니다.\\n\\n2023년 7월 LK-99라는 초전도체에 대한 논문이 아카이브에 올라왔고, 그 이후로 초전도체 뉴스가 뜨겁습니다. LK-99라는 초전도체 개발 가능성은 초전도체 활용 분야로 관심이 갔고, 즉시 주식시장에 그 관심이 반영되었습니다.올해 시장 최대 테마주는 초전도체 관련 주입니다. 초전도체 응용기술을 가졌다는 어떤 기업은 2023년 7월31일부터 상한가 3회로 사흘만에 2배가 올랐습니다. 거래정지 하루가 있었고, 그 다음날엔 하한가를 쳐서 그야말로 주가가 롤러코스터를 타고 있습니다. 주식투자에 각별한 주의가 필요해 보입니다.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.5162681341171265),\n",
       " NodeWithScore(node=TextNode(id_='66f11f29-7609-4aec-a8c8-9c371ac92408', embedding=None, metadata={'file_path': '../data/1.txt', 'file_name': '1.txt', 'file_type': 'text/plain', 'file_size': 3465, 'creation_date': '2024-02-23', 'last_modified_date': '2024-02-23', 'last_accessed_date': '2024-02-23'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='생성형 인공지능(AI) 업계에서 오픈AI와 패권을 다투는 구글이 ‘오픈소스(개방형) AI’ 진영으로 한 걸음 이동했다. 업계는 챗GPT의 기반 모델인 ‘GPT4’의 어떤 것도 공개하지 않는 오픈AI와 구글의 기술 경쟁이 어떤 결말을 가져올지 주목하고 있다.\\n\\n구글은 21일(현지시간) 거대언어모델(LLM) ‘젬마’를 오픈소스로 공개했다. 이에 따라 개별 연구자나 개발자, 기업, 연구기관 등은 젬마를 자유롭게 활용할 수 있다. 젬마는 구글의 AI 모델 ‘제미나이’의 경량형이라고 생각하면 쉽다. AI 모델의 규모를 판단하는 매개변수가 200억개, 700억개인 두 개의 모델로 공개됐다. 제미나이의 매개변수는 1조 7800억개 이상으로 추정된다.\\n\\n오픈소스는 AI 이외의 분야에서도 정보기술(IT) 개발자들이 널리 공감하는 개념이다. AI 모델을 비롯한 컴퓨터 프로그램의 소스코드 등 모든 개발 자산을 누구나 다운로드할 수 있는 공간에 공유하는 것이다. 생성형 AI 시대에 와서도 오픈소스는 많은 지지를 얻었다. 기존 오픈소스 플랫폼 ‘깃허브’나 오픈소스 AI 모델 플랫폼인 ‘허깅페이스’에 수많은 AI 모델들이 공유됐다. 메타(페이스북)는 지난해 빅테크 중 가장 먼저 ‘라마’(LlaMa)라는 생성 AI 모델을 오픈소스로 공개했다. 국내에선 업스테이지의 ‘솔라’, 모레의 ‘모모’ 등이 오픈소스 AI 중 높은 성능 순위를 기록했다.\\n\\n오픈소스 AI는 수많은 전문가들이 모델의 강약점을 함께 고민하고 성능과 안전성을 개선할 수 있다. 특히 기본 모델을 공유하면 가난한 국가나 영세 기관, 개인 개발자 등도 이를 기반으로 AI 서비스를 개발하거나 변형해 새 AI 모델도 만들 수 있다. AI 시대 기술 불평등, 종속 문제를 해소할 ‘AI 민주화’ 방안으로도 꼽힌다.\\n\\n오픈AI는 GPT4의 매개변수조차 공개하지 않는 등 폐쇄 정책으로 일관하고 있다. AI 시대 초반에 기술 우위를 유지해 구글에 앞서기 위해서다. 지금까지는 효과적이다. 최근엔 단 몇 줄의 명령어로 영화 속 한 장면 같은 동영상을 만들어 내는 생성 AI ‘소라’를 공개해 업계를 놀라게 했다.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.47121891379356384)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.source_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "240222_env",
   "language": "python",
   "name": "240222_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
